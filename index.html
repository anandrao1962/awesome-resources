<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Awesome Resources</title>
</head>
<body>
    <h1>Welcome to Anand S. Rao's Awesome Resources</h1>
    <p>Here are some carefully curated resources for learning and mastering AI, ML, and Data Science:</p>
    <ul>
        <li><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">100 Days of ML Code</a></li>
        <li><a href="https://github.com/microsoft/Data-Science-For-Beginners?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">Data Science for Beginners</a></li>
        <li><a href="https://github.com/academic/awesome-datascience?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">Awesome Data Science</a></li>
        <li><a href="https://github.com/datasciencemasters/go?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">Data Science Masters</a></li>
        <li><a href="https://github.com/owainlewis/awesome-artificial-intelligence?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">Awesome AI</a></li>
        <li><a href="https://github.com/alexeygrigorev/data-science-interviews?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">Data Science Interviews</a></li>
        <li><a href="https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">500 AI ML DL CV NLP Projects with Code</a></li>
        <li><a href="https://github.com/khangich/machine-learning-interview?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">Machine Learning Interview</a></li>
        <li><a href="https://github.com/tirthajyoti/Data-science-best-resources?tab=readme-ov-file&utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=top-10-github-repositories-to-master-ai-machine-learning-and-data-science" target="_blank">Data Science Best Resources</a></li>
    </ul>
    <h2>Model Evaluation Papers</h2>
    <ul>
        <li><a href="https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers" target="_blank">Awesome LLMs Evaluation Papers</a></li>
    </ul>
    <h2>Key Research Papers of 2024</h2>
    <h3>January 2024</h3>
        <ul>
            <li>1 Jan, <a href="https://arxiv.org/abs/2401.00788" target="_blank">Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models</a></li>
            <li>2 Jan, <a href="https://arxiv.org/abs/2401.01286" target="_blank">A Comprehensive Study of Knowledge Editing for Large Language Models</a></li>
            <li>2 Jan, <a href="https://arxiv.org/abs/2401.01325" target="_blank">LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</a></li>
            <li>2 Jan, <a href="https://arxiv.org/abs/2401.01335" target="_blank">Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</a></li>
            <li>2 Jan, <a href="https://arxiv.org/abs/2401.01055" target="_blank">LLaMA Beyond English: An Empirical Study on Language Capability Transfer</a></li>
            <li>3 Jan, <a href="https://arxiv.org/abs/2401.01967" target="_blank">A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity</a></li>
            <li>4 Jan, <a href="https://arxiv.org/abs/2401.02415" target="_blank">LLaMA Pro: Progressive LLaMA with Block Expansion</a></li>
            <li>4 Jan, <a href="https://arxiv.org/abs/2401.02412" target="_blank">LLM Augmented LLMs: Expanding Capabilities through Composition</a></li>
            <li>4 Jan, <a href="https://arxiv.org/abs/2401.02994" target="_blank">Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM</a></li>
            <li>5 Jan, <a href="https://arxiv.org/abs/2401.02954" target="_blank">DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</a></li>
            <li>5 Jan, <a href="https://arxiv.org/abs/2401.02957" target="_blank">Denoising Vision Transformers</a></li>
            <li>7 Jan, <a href="https://arxiv.org/abs/2401.03462" target="_blank">Soaring from 4K to 400K: Extending LLM’s Context with Activation Beacon</a></li>
            <li>8 Jan, <a href="https://arxiv.org/abs/2401.04088" target="_blank">Mixtral of Experts</a></li>
            <li>8 Jan, <a href="https://arxiv.org/abs/2401.04081" target="_blank">MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts</a></li>
            <li>8 Jan, <a href="https://arxiv.org/abs/2401.04056" target="_blank">A Minimaximalist Approach to Reinforcement Learning from Human Feedback</a></li>
            <li>9 Jan, <a href="https://arxiv.org/abs/2401.04679" target="_blank">RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</a></li>
            <li>10 Jan, <a href="https://arxiv.org/abs/2401.05566" target="_blank">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</a></li>
            <li>11 Jan, <a href="https://arxiv.org/abs/2401.06104" target="_blank">Transformers are Multi-State RNNs</a></li>
            <li>11 Jan, <a href="https://arxiv.org/abs/2401.06091" target="_blank">A Closer Look at AUROC and AUPRC under Class Imbalance</a></li>
            <li>12 Jan, <a href="https://arxiv.org/abs/2401.06692" target="_blank">An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models</a></li>
            <li>16 Jan, <a href="https://arxiv.org/abs/2401.08565" target="_blank">Tuning Language Models by Proxy</a></li>
    <!-- Add more as needed -->
        </ul>
    <h3>February 2024</h3>
        <ul>
            <li>1 Feb, <a href="https://arxiv.org/abs/2402.00396" target="_blank">Efficient Exploration for LLMs</a></li>
            <li>1 Feb, <a href="https://arxiv.org/abs/2402.00838" target="_blank">OLMo: Accelerating the Science of Language Models</a></li>
            <li>1 Feb, <a href="https://arxiv.org/abs/2402.00841" target="_blank">Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?</a></li>
            <li>1 Feb, <a href="https://arxiv.org/abs/2402.01032" target="_blank">Repeat After Me: Transformers are Better than State Space Models at Copying</a></li>
            <li>2 Feb, <a href="https://arxiv.org/abs/2402.01878" target="_blank">LiPO: Listwise Preference Optimization through Learning-to-Rank</a></li>
            <li>2 Feb, <a href="https://arxiv.org/abs/2402.01355" target="_blank">FindingEmo: An Image Dataset for Emotion Recognition in the Wild</a></li>
            <li>3 Feb, <a href="https://arxiv.org/abs/2402.05120" target="_blank">More Agents Is All You Need</a></li>
            <!-- Add more as needed -->
        </ul>
    <footer>
        <p>© 2024 Anand S. Rao. All rights reserved.</p>
    </footer>
</body>
</html>
